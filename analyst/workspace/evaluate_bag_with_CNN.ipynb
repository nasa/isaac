{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da96116-06fe-42b8-8c3c-1a4605855239",
   "metadata": {},
   "source": [
    "Classify images in bag using trained CNN\n",
    "=============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24eb188-ee78-4de8-a7c8-f1001b0e65ba",
   "metadata": {},
   "source": [
    "This notebook looks at all the images in a bagfile and clasifies them. It needs prespecified target variables. If you don't have any you can select a target with the target select notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c32350-ad27-443f-91f0-fc9b0e7a93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import quaternion\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import torch\n",
    "from lightglue import SuperPoint\n",
    "from lightglue.utils import load_image\n",
    "\n",
    "import scripts.query_image as query_image\n",
    "import scripts.save_patch as save_patch\n",
    "import scripts.divide_data as divide_data\n",
    "import scripts.points_and_polygons as points_and_polygons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a672821b-b2d6-4dd7-a00f-96fa294ecefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "ros_topic_pose = \"/gnc/ekf\".replace(\"/\", \"_\")[1:]\n",
    "ros_topic_image = \"/hw/cam_sci/info\".replace(\"/\", \"_\")[1:]\n",
    "\n",
    "# Target Pose\n",
    "target_position = np.array([-0.84, 0.6, -0.81])\n",
    "target_attitude = quaternion.from_euler_angles(0, 0, np.radians(180))  # order is roll, pitch, yaw\n",
    "\n",
    "# Query Contraints\n",
    "max_distance = 1.0\n",
    "min_distance = 0.2\n",
    "max_angle = 30\n",
    "\n",
    "# Target size, the target is pointed to in the x-axis\n",
    "target_size_y = 0.05\n",
    "target_size_z = 0.07\n",
    "\n",
    "base_image_path = 'data/bags/2024-03-21_tim/bsharp/Fixed/isaac_sci_cam_image_delayed/1711061089.228.jpg'\n",
    "image_path = 'data/bags/2024-03-21_tim/bsharp/Fixed/isaac_sci_cam_image_delayed/'\n",
    "bag = '20240321_2254_survey_bsharp7_1.fix_all.bag'\n",
    "target_corners = [[(1838, 1531), (2050, 1531), (1838, 1683), (2050, 1694)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59fd1533-44c8-4c4a-a14d-c7d3e87064b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to isaac database\n",
      "From database got 35 matches\n",
      "From first filtering got 25 matches\n",
      "Query successful, got 25 matches\n"
     ]
    }
   ],
   "source": [
    "result = query_image.query_image_of_bag(target_position, target_attitude, ros_topic_pose, ros_topic_image, max_distance, min_distance, max_angle, target_size_y, target_size_z, bag) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e16644-ded1-421e-8c56-f7a82def3792",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check if Nvidia CUDA is supported by the gpu otherwise set device to cpu\n",
    "base_image = load_image(base_image_path)\n",
    "extractor = SuperPoint(max_num_keypoints=2048).eval().to(device)  # Load Superpoint as the extractor\n",
    "feats_base_image = extractor.extract(base_image.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef64831e-8577-4d06-926a-9d851a650aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_dictionary = {}\n",
    "corners = points_and_polygons.identify_corners(target_corners)\n",
    "\n",
    "# Go through all the images in the bag file and store the classification in a dictionary.\n",
    "for idx, element in enumerate(result): \n",
    "    \n",
    "    transformed_image = save_patch.match_images_and_transform(base_image_path, image_path + element['img'], feats_base_image)\n",
    "    extracted_image_patch = save_patch.extract_image(transformed_image, [corners[0]['A'], corners[0]['B'], corners[0]['D'], corners[0]['C']])\n",
    "                                            \n",
    "    \n",
    "    # Parameters\n",
    "    classes = ['off', 'on']  # specify the image classes\n",
    "    model_name = 'switch_model_cnn.pt'  # saved model name\n",
    "    \n",
    "    \n",
    "    # Open image\n",
    "    image = Image.fromarray(extracted_image_patch)\n",
    "    \n",
    "    # Open model\n",
    "    model = models.densenet121(weights='DenseNet121_Weights.DEFAULT')\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(1024, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(256, 2),\n",
    "        nn.LogSoftmax(dim=1),\n",
    "    )\n",
    "    model.load_state_dict(torch.load('switch_model_cnn.pt'))\n",
    "    model.eval()\n",
    "    \n",
    "    # Classify Image!\n",
    "    test_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    image_tensor = test_transforms(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    output = model(image_tensor)\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "    \n",
    "    # Get the predicted class and its probability\n",
    "    _, predicted = torch.max(probabilities, 1)\n",
    "    confidence = probabilities[0][predicted.item()].item()\n",
    "\n",
    "    classification_dictionary.update({element['img']: classes[predicted.item()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22eb005-9aa8-4faf-a029-05175e35211b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1711061763.930.jpg': 'on', '1711061764.596.jpg': 'on', '1711061765.480.jpg': 'on', '1711061765.897.jpg': 'on', '1711061766.355.jpg': 'on', '1711061766.814.jpg': 'on', '1711061767.228.jpg': 'on', '1711061767.646.jpg': 'on', '1711061768.056.jpg': 'on', '1711061768.557.jpg': 'on', '1711061769.035.jpg': 'on', '1711061769.627.jpg': 'on', '1711061770.228.jpg': 'on', '1711061770.889.jpg': 'on', '1711061771.349.jpg': 'on', '1711061771.777.jpg': 'on', '1711061772.410.jpg': 'on', '1711061772.826.jpg': 'on', '1711061773.284.jpg': 'on', '1711061773.960.jpg': 'on', '1711061774.668.jpg': 'on', '1711061775.167.jpg': 'on', '1711061775.784.jpg': 'on', '1711061776.409.jpg': 'on', '1711061776.826.jpg': 'on'}\n"
     ]
    }
   ],
   "source": [
    "print(classification_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e2e26-230e-4c91-a8b9-d00d4d457224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
